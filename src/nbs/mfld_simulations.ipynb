{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code to run simulations for feature learning in various infinite-width neural network regimes. In particular, we focus on:\n",
    "1. Mean field\n",
    "2. Mean field Langevin dynamics\n",
    "3. Neural tangent kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning k-sparse parities (refer to [Suzuki et al., 2023](https://proceedings.neurips.cc/paper_files/paper/2023/hash/6cc321baf0a8611b1d1bdbd18822667b-Abstract-Conference.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the XOR problem where $k=2$ (Wei et al., 2019), we can generate our data from hypercube $\\{ \\pm 1 / \\sqrt{d} \\}^d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_xor_data(n, d):\n",
    "    # Generate data from {Â±1/sqrt(d)}^d and labels as XOR of first 2 features.\n",
    "    X = cp.random.choice([1, -1], size=(n, d)) / cp.sqrt(d)\n",
    "    y = cp.sign(X[:, 0] * X[:, 1])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we define our neuron $h_z(x)$ with parameters $x = (x_1, x_2, x_3) \\in \\mathbb{R}^{d + 1 + 1}$ as such:\n",
    "$$h_{x}(z) = \\bar{R} \\cdot \\tanh(z^T x_1 + x_2) + \\frac{2 \\tanh(x_3)}{3}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_activation(x, z, R_bar=15):\n",
    "    # x is in R^(d+2): x[:d] are weights, x[d] is a bias for tanh(z^T x[:d]), and x[d+1] is a further bias.\n",
    "    d = len(z)\n",
    "    a = cp.dot(z, x[:d]) + x[d]\n",
    "    b = x[d+1]\n",
    "    return R_bar * (cp.tanh(a) + 2 * cp.tanh(b)) / 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some derivatives for grad calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtanh(x):\n",
    "    return 1.0 - cp.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_grad(x, z, R_bar=15):\n",
    "    # Compute gradients of h_x with respect to the parameters x.\n",
    "    d = len(z)\n",
    "    a = cp.dot(z, x[:d]) + x[d]\n",
    "    b = x[d+1]\n",
    "    grad_x1 = (R_bar/3.0) * dtanh(a) * z          # gradient w.r.t. x1\n",
    "    grad_bias = cp.array([R_bar/3.0 * dtanh(a)])      # gradient w.r.t. bias x[d]\n",
    "    grad_bias2 = cp.array([2*R_bar/3.0 * dtanh(b)])   # gradient w.r.t. bias x[d+1]\n",
    "    return cp.concatenate([grad_x1, grad_bias, grad_bias2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(f, y):\n",
    "    return cp.log(1 + cp.exp(-y * f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_logistic_loss(f, y):\n",
    "    return - y / (1 + cp.exp(y * f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our neural network formulation: \n",
    "\n",
    "$$f(x; \\theta) = \\frac{1}{M} \\sum_{i=1}^{M} \\sigma \\langle x, \\theta_i \\rangle$$\n",
    "\n",
    "where $\\sigma \\langle x, \\theta_i \\rangle = h_{x}(z_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(particles, z, R_bar=15):\n",
    "    outputs = cp.array([neuron_activation(x, z, R_bar) for x in particles])\n",
    "    return cp.mean(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(particles, X, y, R_bar=15):\n",
    "    preds = cp.array([cp.sign(predict(particles, x, R_bar)) for x in X])\n",
    "    return cp.mean(preds == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below follows from our update function in MFLD:\n",
    "\n",
    "$$dX_t = - \\nabla \\frac{\\delta F(\\mu_t)}{\\delta \\mu}(X_t)dt + \\sqrt{2 \\lambda}dW_t$$\n",
    "\n",
    "Recall:\n",
    "$\\frac{\\delta F(\\mu)}{\\delta \\mu}(x) = \\frac{1}{n} \\sum_{i=1}^{n} l'(y_i f_{\\mu}(z_i))y_i h_x(z_i) + \\lambda (\\lambda_1 \\|x\\|^2)$\n",
    "\n",
    "Hence:\n",
    "\n",
    "$X_{\\tau+1}^{i} = X_{\\tau}^{i} - \\eta \\nabla \\frac{\\delta F(\\mu_r)}{\\delta \\mu}(X_r^i) + \\sqrt{2\\lambda\\eta} \\xi_{\\tau}^{i}$\n",
    "\n",
    "where $\\xi_{\\tau}^{i} \\sim N(0, I)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_particles(particles, X, y, eta, lam, lambda_1, R_bar=15):\n",
    "    \"\"\"\n",
    "    Note that the update step follows from eq (4) of (Suzuki et al., 2023)\n",
    "    Our L2 regularization update is implicity included in \\nabla \\frac{\\delta F(\\mu_i)}}{\\delta \\mu} of eq (4)\n",
    "    args:\n",
    "        lam: denotes the variance of the noise updates in Langevin dynamics (i.e. \\sqrt{2 \\lambda \\eta} W, W \\sim Brownian)\n",
    "        lambda_1: denotes strength of L2 reg.\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    N = len(particles)\n",
    "    # Compute network output for each data point\n",
    "    f_preds = cp.zeros(n)\n",
    "    for i in range(n):\n",
    "        f_preds[i] = cp.mean(cp.array([neuron_activation(x, X[i], R_bar) for x in particles]))\n",
    "    # Compute derivative of the loss for each data point\n",
    "    loss_derivs = cp.array([d_logistic_loss(f_preds[i], y[i]) for i in range(n)])\n",
    "    \n",
    "    new_particles = []\n",
    "    for x in particles:\n",
    "        grad_loss = cp.zeros_like(x)\n",
    "        # Accumulate gradient contributions from all training data\n",
    "        for i in range(n):\n",
    "            grad_loss += (loss_derivs[i] * y[i]) * neuron_grad(x, X[i], R_bar)\n",
    "        grad_loss = grad_loss / n\n",
    "        grad_reg = 2 * lambda_1 * x\n",
    "        grad = grad_loss + grad_reg\n",
    "        noise = cp.random.randn(*x.shape)\n",
    "        x_new = x - eta * grad + cp.sqrt(2 * lam * eta) * noise\n",
    "        new_particles.append(x_new)\n",
    "    return new_particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall: the annealing process described in [Suzuki et al., 2023]. Convergence rate is dependent on LSI constant hence by decreasing $\\lambda$,\n",
    " \n",
    "$\\lambda^{(k)} = 2^{(-\\kappa)}\\lambda^{(0)}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(d=20, n_train=500, n_test=200, \n",
    "                   num_particles=1000, eta=0.05, T_per_round=200, \n",
    "                   num_rounds=6, lam_init=0.1, R_bar=15):\n",
    "    # Generate training and test data\n",
    "    X_train, y_train = generate_xor_data(n_train, d)\n",
    "    X_test, y_test = generate_xor_data(n_test, d)\n",
    "    # Initialize particles randomly\n",
    "    particles = [cp.random.randn(d+2) for _ in range(num_particles)]\n",
    "    \n",
    "    accuracy_history = []\n",
    "    lam_history = []\n",
    "    \n",
    "    lam_current = lam_init\n",
    "    lambda_1_value = 0.1 # Suzuki et al., 2023\n",
    "    for round in range(num_rounds):\n",
    "        print(f\"Annealing Round {round+1}, lambda = {lam_current:.5f}\")\n",
    "        for t in range(T_per_round):\n",
    "            particles = update_particles(particles, X_train, y_train, eta, lam_current, lambda_1_value, R_bar)\n",
    "        acc = evaluate_accuracy(particles, X_test, y_test, R_bar)\n",
    "        print(f\"Test accuracy after round {round+1}: {acc:.3f}\")\n",
    "        accuracy_history.append(acc)\n",
    "        lam_history.append(lam_current)\n",
    "        lam_current *= 0.5  # Anneal: reduce lambda by factor of 2.\n",
    "    return accuracy_history, lam_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation_and_store(d=20, n_train=500, n_test=200,\n",
    "                             num_particles=1000, eta=0.05, T_per_round=200,\n",
    "                             num_rounds=6, lam_init=0.1, lambda_1=0.1, R_bar=15):\n",
    "\n",
    "    X_train, y_train = generate_xor_data(n_train, d)\n",
    "    X_test, y_test = generate_xor_data(n_test, d)\n",
    "    particles = [cp.random.randn(d+2, dtype=cp.float32) for _ in range(num_particles)]\n",
    "\n",
    "    particle_history = {} # Dictionary to store particle states\n",
    "    accuracy_history = []\n",
    "    lam_history = []\n",
    "\n",
    "    print(\"Storing initial particle state...\")\n",
    "    particle_history[0] = [p.copy() for p in particles] # Store initial state (round 0)\n",
    "\n",
    "    lam_current = lam_init\n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"Annealing Round {round_num+1}, lambda = {lam_current:.5f}\")\n",
    "        for t in range(T_per_round):\n",
    "            # Pass lambda_1 to update_particles\n",
    "            particles = update_particles(particles, X_train, y_train, eta, lam_current, lambda_1, R_bar)\n",
    "            print(f\"Finished timestep: {t+1} / {T_per_round}\")\n",
    "            # Optional: could store particles more frequently within a round if needed\n",
    "            # if (t+1) % 50 == 0: print(f\"  Iteration {t+1}/{T_per_round}\")\n",
    "\n",
    "        # Store particles after the round\n",
    "        print(f\"Storing particle state after round {round_num+1}...\")\n",
    "        particle_history[round_num+1] = [p.copy() for p in particles]\n",
    "\n",
    "        # Evaluate accuracy\n",
    "        acc = evaluate_accuracy(particles, X_test, y_test, R_bar)\n",
    "        # Transfer acc to CPU if it's a cupy scalar\n",
    "        acc_cpu = cp.asnumpy(acc) if isinstance(acc, cp.ndarray) else acc\n",
    "        print(f\"Test accuracy after round {round_num+1}: {acc_cpu:.4f}\")\n",
    "        accuracy_history.append(acc_cpu)\n",
    "        lam_history.append(lam_current)\n",
    "\n",
    "        # Anneal\n",
    "        lam_current *= 0.5\n",
    "\n",
    "    return particle_history, accuracy_history, lam_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, I am slightly confused why Suzuki et al., 2023 choose $\\bar R = 15$ for their experimental results when they mention in paper that $\\bar R = k$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing initial particle state...\n",
      "Annealing Round 1, lambda = 0.10000\n",
      "Finished timestep: 0 / 50\n",
      "Finished timestep: 1 / 50\n",
      "Finished timestep: 2 / 50\n",
      "Finished timestep: 3 / 50\n",
      "Finished timestep: 4 / 50\n",
      "Finished timestep: 5 / 50\n",
      "Finished timestep: 6 / 50\n",
      "Finished timestep: 7 / 50\n",
      "Finished timestep: 8 / 50\n",
      "Finished timestep: 9 / 50\n",
      "Finished timestep: 10 / 50\n",
      "Finished timestep: 11 / 50\n",
      "Finished timestep: 12 / 50\n",
      "Finished timestep: 13 / 50\n",
      "Finished timestep: 14 / 50\n",
      "Finished timestep: 15 / 50\n",
      "Finished timestep: 16 / 50\n",
      "Finished timestep: 17 / 50\n",
      "Finished timestep: 18 / 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m cp.random.seed(\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m particle_history, accuracy_history, lam_history = \u001b[43mrun_simulation_and_store\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43md\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_test\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_particles\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Reduced particles for faster testing\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43meta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_per_round\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Reduced steps for faster testing\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam_init\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlambda_1\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Provide lambda_1\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mR_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Plot the test accuracy evolution over annealing rounds\u001b[39;00m\n\u001b[32m     13\u001b[39m plt.figure(figsize=(\u001b[32m8\u001b[39m,\u001b[32m6\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mrun_simulation_and_store\u001b[39m\u001b[34m(d, n_train, n_test, num_particles, eta, T_per_round, num_rounds, lam_init, lambda_1, R_bar)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnnealing Round \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mround_num+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, lambda = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlam_current\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T_per_round):\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Pass lambda_1 to update_particles\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     particles = \u001b[43mupdate_particles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparticles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam_current\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinished timestep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mT_per_round\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Optional: could store particles more frequently within a round if needed\u001b[39;00m\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# if (t+1) % 50 == 0: print(f\"  Iteration {t+1}/{T_per_round}\")\u001b[39;00m\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Store particles after the round\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mupdate_particles\u001b[39m\u001b[34m(particles, X, y, eta, lam, lambda_1, R_bar)\u001b[39m\n\u001b[32m     12\u001b[39m f_preds = cp.zeros(n)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     f_preds[i] = cp.mean(cp.array(\u001b[43m[\u001b[49m\u001b[43mneuron_activation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_bar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparticles\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Compute derivative of the loss for each data point\u001b[39;00m\n\u001b[32m     16\u001b[39m loss_derivs = cp.array([d_logistic_loss(f_preds[i], y[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     12\u001b[39m f_preds = cp.zeros(n)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     f_preds[i] = cp.mean(cp.array([\u001b[43mneuron_activation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_bar\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m particles]))\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Compute derivative of the loss for each data point\u001b[39;00m\n\u001b[32m     16\u001b[39m loss_derivs = cp.array([d_logistic_loss(f_preds[i], y[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mneuron_activation\u001b[39m\u001b[34m(x, z, R_bar)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mneuron_activation\u001b[39m(x, z, R_bar=\u001b[32m15\u001b[39m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# x is in R^(d+2): x[:d] are weights, x[d] is a bias for tanh(z^T x[:d]), and x[d+1] is a further bias.\u001b[39;00m\n\u001b[32m      3\u001b[39m     d = \u001b[38;5;28mlen\u001b[39m(z)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     a = \u001b[43mcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m + x[d]\n\u001b[32m      5\u001b[39m     b = x[d+\u001b[32m1\u001b[39m]\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m R_bar * (cp.tanh(a) + \u001b[32m2\u001b[39m * cp.tanh(b)) / \u001b[32m3.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/452_env/lib/python3.11/site-packages/cupy/linalg/_product.py:63\u001b[39m, in \u001b[36mdot\u001b[39m\u001b[34m(a, b, out)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns a dot product of two arrays.\u001b[39;00m\n\u001b[32m     45\u001b[39m \n\u001b[32m     46\u001b[39m \u001b[33;03mFor arrays with more than one axis, it computes the dot product along the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# TODO(okuta): check type\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "cp.random.seed(42)\n",
    "\n",
    "particle_history, accuracy_history, lam_history = run_simulation_and_store(\n",
    "    d=20, n_train=500, n_test=200,\n",
    "    num_particles=200, # Reduced particles for faster testing\n",
    "    eta=0.05, T_per_round=50, # Reduced steps for faster testing\n",
    "    num_rounds=6, lam_init=0.1,\n",
    "    lambda_1=0.1, # Provide lambda_1\n",
    "    R_bar=15\n",
    ")\n",
    "\n",
    "# Plot the test accuracy evolution over annealing rounds\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(cp.arange(1, len(accuracy_history)+1), accuracy_history, marker='o')\n",
    "plt.xlabel('Annealing Round')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Test Accuracy over Annealing Rounds')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot lambda evolution versus annealing round\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(cp.arange(1, len(lam_history)+1), lam_history, marker='o', color='r')\n",
    "plt.xlabel('Annealing Round')\n",
    "plt.ylabel('Lambda')\n",
    "plt.title('Regularization Parameter over Annealing Rounds')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there is any way to visualize the feature learning here. Perhaps we can PCA the neuron weights and visualize how they might change with training (following similar methodology to what was presented in Yang et al., 2022 in their abc-param paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA neuron weights over time\n",
    "\n",
    "print(\"\\n--- Generating PCA Visualization ---\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "num_rounds_to_plot = len(particle_history)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_rounds_to_plot))\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "scaler = StandardScaler() \n",
    "\n",
    "for round_num, particles_cp in particle_history.items():\n",
    "    print(f\"Processing PCA for round {round_num}\")\n",
    "\n",
    "    particle_matrix_cp = cp.stack(particles_cp) # dims: (num_particles, d+2)\n",
    "\n",
    "    # Transfer to CPU for scikit-learn\n",
    "    particle_matrix_np = cp.asnumpy(particle_matrix_cp)\n",
    "\n",
    "    particle_matrix_scaled = scaler.fit_transform(particle_matrix_np)\n",
    "\n",
    "    # Fit PCA and transform data\n",
    "    pca.fit(particle_matrix_scaled)\n",
    "    particles_pca = pca.transform(particle_matrix_scaled)\n",
    "\n",
    "    # Plot\n",
    "    plt.scatter(particles_pca[:, 0], particles_pca[:, 1],\n",
    "                alpha=0.6, label=f'Round {round_num}', color=colors[round_num], s=10)\n",
    "\n",
    "plt.title('PCA of MFLD Particle Parameters Over Annealing Rounds')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how certain weights change over time\n",
    "\n",
    "print(\"\\n--- Generating Weight Component Visualization ---\")\n",
    "# compare initial and final rounds\n",
    "rounds_to_compare = [0, len(particle_history) - 1]\n",
    "d_value = 20\n",
    "num_bins = 50\n",
    "\n",
    "fig, axes = plt.subplots(1, len(rounds_to_compare), figsize=(12, 5), sharey=True)\n",
    "fig.suptitle('Distribution of Relevant vs. Irrelevant Weight Components')\n",
    "\n",
    "for i, round_num in enumerate(rounds_to_compare):\n",
    "    print(f\"Processing weight distributions for round {round_num}\")\n",
    "    ax = axes[i]\n",
    "    particles_cp = particle_history[round_num]\n",
    "    particle_matrix_cp = cp.stack(particles_cp) # shape (num_particles, d+2)\n",
    "\n",
    "    # Extract weight vectors (first d components)\n",
    "    weights_cp = particle_matrix_cp[:, :d_value]\n",
    "\n",
    "    # Separate relevant (first 2) and irrelevant weights\n",
    "    # recalll, XOR problem looks at first two input dimensions (perhaps first two feature weights are also changed more)\n",
    "    relevant_weights_cp = weights_cp[:, :2].flatten()\n",
    "    irrelevant_weights_cp = weights_cp[:, 2:].flatten()\n",
    "\n",
    "    # Transfer to CPU for plotting\n",
    "    relevant_weights_np = cp.asnumpy(relevant_weights_cp)\n",
    "    irrelevant_weights_np = cp.asnumpy(irrelevant_weights_cp)\n",
    "\n",
    "    # Plot histograms\n",
    "    bin_min = min(relevant_weights_np.min(), irrelevant_weights_np.min())\n",
    "    bin_max = max(relevant_weights_np.max(), irrelevant_weights_np.max())\n",
    "    bins = np.linspace(bin_min, bin_max, num_bins)\n",
    "\n",
    "    ax.hist(relevant_weights_np, bins=bins, alpha=0.7, label='Relevant Weights (Dims 1-2)', density=True)\n",
    "    ax.hist(irrelevant_weights_np, bins=bins, alpha=0.7, label=f'Irrelevant Weights (Dims 3-{d_value})', density=True)\n",
    "    ax.set_title(f'Round {round_num}')\n",
    "    ax.set_xlabel('Weight Value')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Generating Scatter Plot of Relevant Weights ---\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for round_num, particles_cp in particle_history.items():\n",
    "    print(f\"Processing scatter plot for round {round_num}\")\n",
    "    particle_matrix_cp = cp.stack(particles_cp) # shape (num_particles, d+2)\n",
    "    weights_cp = particle_matrix_cp[:, :d_value]\n",
    "    relevant_weights_cp = weights_cp[:, :2] # Shape (num_particles, 2)\n",
    "\n",
    "    # Transfer to CPU\n",
    "    relevant_weights_np = cp.asnumpy(relevant_weights_cp)\n",
    "\n",
    "    # Plot\n",
    "    plt.scatter(relevant_weights_np[:, 0], relevant_weights_np[:, 1],\n",
    "                alpha=0.3, label=f'Round {round_num}', color=colors[round_num], s=10)\n",
    "\n",
    "plt.title('Scatter Plot of First Two Weight Components Over Annealing Rounds')\n",
    "plt.xlabel('Weight Component 1')\n",
    "plt.ylabel('Weight Component 2')\n",
    "# plt.legend()\n",
    "plt.grid(True)\n",
    "plt.axhline(0, color='grey', lw=0.5)\n",
    "plt.axvline(0, color='grey', lw=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "452_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
